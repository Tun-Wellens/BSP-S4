{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc944d4a-02ea-4625-8059-86b8b5fc4f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_dataset, Audio\n",
    "import pandas as pd\n",
    "import os\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Audio as IPyAudio\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\"\"\"Files need to be downloaded manually from the corresponding sources,\n",
    "    for further informations check the paper, which point to the sources.\"\"\"\n",
    "\n",
    "\n",
    "### 1. Load MAILABS (x-lb) MBARNIG ###\n",
    "def load_mailabs(root):\n",
    "    entries = []\n",
    "    for gender in [\"male\", \"female\"]:\n",
    "        gender_path = os.path.join(root, gender)\n",
    "        if not os.path.isdir(gender_path):\n",
    "            continue\n",
    "        for speaker in os.listdir(gender_path):\n",
    "            speaker_path = os.path.join(gender_path, speaker)\n",
    "            wav_dir = os.path.join(speaker_path, \"wavs\")\n",
    "            metadata_file = os.path.join(speaker_path, \"metadata.csv\")\n",
    "            if os.path.exists(metadata_file):\n",
    "                df = pd.read_csv(metadata_file, sep=\"|\", header=None, names=[\"filename\", \"transcription\", \"_\"])\n",
    "                for _, row in df.iterrows():\n",
    "                    wav_path = os.path.join(wav_dir, row[\"filename\"] + \".wav\")\n",
    "                    audio, sr = sf.read(wav_path)\n",
    "                    audio = audio.astype(np.float32)\n",
    "                    entries.append({\n",
    "                        \"audio\": {\"array\": audio, \"sampling_rate\": sr},\n",
    "                        \"transcription\": row[\"transcription\"],\n",
    "                        \"source\": \"Mailabs\"\n",
    "                    })\n",
    "    return Dataset.from_list(entries)\n",
    "\n",
    "mailabs_ds = load_mailabs(\"./lb-de-fr-en-pt-12800-TTS-CORPUS/mailabs/x-lb/by_book\")\n",
    "print(f\" Loaded {len(mailabs_ds)} samples from Mailabs\")\n",
    "\n",
    "# Load FLEURS dataset\n",
    "raw_fleurs = load_dataset(\"google/fleurs\", \"lb_lu\", split=\"train\")\n",
    "raw_fleurs = raw_fleurs.cast_column(\"audio\", Audio(decode=True))\n",
    "\n",
    "fleurs_entries = []\n",
    "for sample in raw_fleurs:\n",
    "    fleurs_entries.append({\n",
    "        \"audio\": {\n",
    "            \"array\": np.array(sample[\"audio\"][\"array\"], dtype=np.float32),\n",
    "            \"sampling_rate\": sample[\"audio\"][\"sampling_rate\"]\n",
    "        },\n",
    "        \"transcription\": sample[\"raw_transcription\"],\n",
    "        \"source\": \"FLEURS\"\n",
    "    })\n",
    "\n",
    "fleurs_ds = Dataset.from_list(fleurs_entries)\n",
    "print(f\" Loaded {len(fleurs_ds)} samples from FLEURS\")\n",
    "\n",
    "### 3. Load RTL ###\n",
    "def load_rtl(path):\n",
    "    entries = []\n",
    "    for split in [\"dev\", \"test\"]:\n",
    "        tsv_path = os.path.join(path, split, f\"{split}.tsv\")\n",
    "        df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "        for _, row in df.iterrows():\n",
    "            audio_path = os.path.join(path, split, row[\"filename\"])\n",
    "            audio, sr = librosa.load(audio_path, sr=None)\n",
    "            audio = audio.astype(np.float32)\n",
    "            entries.append({\n",
    "                \"audio\": {\"array\": audio, \"sampling_rate\": sr},\n",
    "                \"transcription\": row[\"transcription\"],\n",
    "                \"source\": \"RTL\"\n",
    "            })\n",
    "    return Dataset.from_list(entries)\n",
    "\n",
    "rtl_ds = load_rtl(\"./rtl\")\n",
    "print(f\" Loaded {len(rtl_ds)} samples from RTL\")\n",
    "\n",
    "### 4. Combine\n",
    "dataset_parts = [fleurs_ds, rtl_ds, mailabs_ds]\n",
    "full_dataset = concatenate_datasets(dataset_parts)\n",
    "print(f\" Final combined dataset has {len(full_dataset)} samples.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce84bf-c0e5-4d43-940b-b114a5364a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(f\" Loaded {len(full_dataset)} samples from combined dataset.\")\n",
    "\n",
    "# Visualize and play one sample from full_dataset\n",
    "def show_example(index):\n",
    "    sample = full_dataset[index]\n",
    "\n",
    "    print(f\"\\n Sample {index}\")\n",
    "    print(f\" Source: {sample['source']}\")\n",
    "    print(f\" Transcript:\\n{sample['transcription']}\")\n",
    "\n",
    "    audio = sample[\"audio\"][\"array\"]\n",
    "    sr = sample[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "    # Plot waveform\n",
    "    plt.figure(figsize=(12, 2.5))\n",
    "    plt.plot(np.linspace(0, len(audio) / sr, num=len(audio)), audio)\n",
    "    plt.title(\"Waveform\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Audio playback\n",
    "    display(IPyAudio(audio, rate=sr))\n",
    "\n",
    "# Interactive slider\n",
    "index_widget = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=len(full_dataset) - 1,\n",
    "    step=1,\n",
    "    description='Sample:',\n",
    "    continuous_update=False\n",
    ")\n",
    "\n",
    "widgets.interact(show_example, index=index_widget)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc44e6d-e0cf-4b76-b460-ccc777646ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio, concatenate_datasets\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "import torch\n",
    "from jiwer import wer as jiwer_wer\n",
    "import wandb\n",
    "import os\n",
    "from IPython.display import Audio as PlayAudio, display #remove later\n",
    "\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# Configuration\n",
    "learning_rate = 3e-5\n",
    "run_name = \"\"\n",
    "output_dir = \"\"\n",
    "wandb.init(project=\"\", name=run_name)\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = full_dataset\n",
    "eval_raw = load_dataset(\"google/fleurs\", \"lb_lu\", split=\"validation\")\n",
    "eval_raw = eval_raw.cast_column(\"audio\", Audio(decode=True))\n",
    "eval_dataset = eval_raw\n",
    "\n",
    "# Load model and processor\n",
    "model_name = \"openai/whisper-medium\"\n",
    "processor = WhisperProcessor.from_pretrained(model_name)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Tokenizer setup\n",
    "processor.tokenizer.set_prefix_tokens(language=\"luxembourgish\", task=\"transcribe\")\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"luxembourgish\", task=\"transcribe\")\n",
    "\n",
    "# Freeze first conv layers\n",
    "for param in model.model.encoder.conv1.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.model.encoder.conv2.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Preprocessing\n",
    "def prepare_example(example):\n",
    "    audio = example[\"audio\"]\n",
    "    example[\"input_features\"] = processor(audio[\"array\"], sampling_rate=16000).input_features[0]\n",
    "    transcription = example.get(\"raw_transcription\", example.get(\"transcription\", \"\"))\n",
    "    example[\"labels\"] = processor.tokenizer(transcription, add_special_tokens=True).input_ids #also adds EOS token\n",
    "    return example\n",
    "\n",
    "train_dataset = train_dataset.map(prepare_example, remove_columns=train_dataset.column_names)\n",
    "eval_dataset = eval_dataset.map(prepare_example, remove_columns=eval_dataset.column_names)\n",
    "\n",
    "\n",
    "# Data collator\n",
    "def data_collator(batch):\n",
    "    input_features = torch.tensor([b[\"input_features\"] for b in batch])\n",
    "    \n",
    "    # Pad with a different token temporarily (like -100) so EOS stays untouched\n",
    "    labels_batch = processor.tokenizer.pad(\n",
    "    {\"input_ids\": [b[\"labels\"] for b in batch]},\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "    labels_batch = labels_batch.input_ids\n",
    "    labels_batch[labels_batch == processor.tokenizer.pad_token_id] = -100\n",
    "    return {\"input_features\": input_features, \"labels\": labels_batch}\n",
    "\n",
    "\n",
    "# Metrics\n",
    "normalizer = BasicTextNormalizer()\n",
    "\n",
    "def normalize_text(text):\n",
    "    return normalizer(text.strip())\n",
    "\n",
    "fixed_indices = [22, 56, 7]\n",
    "fixed_examples = [eval_raw[i] for i in fixed_indices]\n",
    "fixed_inputs = []\n",
    "for example in fixed_examples:\n",
    "    inputs = processor(example[\"audio\"][\"array\"], sampling_rate=16000, return_tensors=\"pt\")\n",
    "    inputs[\"input_features\"] = inputs.input_features[0]\n",
    "    text = example.get(\"raw_transcription\", example.get(\"transcription\", \"\"))\n",
    "    fixed_inputs.append({\"input_features\": inputs[\"input_features\"], \"reference\": text})\n",
    "\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    pred_str = processor.tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_ids[label_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "    pred_str_norm = [normalize_text(p) for p in pred_str]\n",
    "    label_str_norm = [normalize_text(l) for l in label_str]\n",
    "    wer = float(jiwer_wer(label_str_norm, pred_str_norm))\n",
    "    print(f\"\\n=== WER: {wer:.4f} ===\")\n",
    "    print(\"\\n--- Fixed sample predictions ---\")\n",
    "    model.eval()\n",
    "    for i, ex in enumerate(fixed_inputs):\n",
    "        with torch.no_grad():\n",
    "            features = ex[\"input_features\"].unsqueeze(0).to(model.device)\n",
    "            gen_ids = model.generate(\n",
    "                features,\n",
    "                max_length=128,\n",
    "                early_stopping=True,\n",
    "                eos_token_id=processor.tokenizer.eos_token_id,\n",
    "                forced_decoder_ids=forced_decoder_ids,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                num_beams=2\n",
    "            )\n",
    "            pred_str = processor.tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "            print(f\"[Sample {i}]\")\n",
    "            print(f\"Reference: {ex['reference']}\")\n",
    "            print(f\"Prediction: {pred_str}\")\n",
    "    wandb.log({\"wer\": wer})\n",
    "    return {\"wer\": wer}\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"\",\n",
    "    report_to=\"wandb\",\n",
    "    run_name=run_name,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=learning_rate,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=500,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26663e4-ee2c-421e-8e56-c7e9fe443f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(training_args.output_dir)\n",
    "processor.push_to_hub(\"Tun-Wellens/whisper-medium-final-test0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
