{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 934/934 [00:04<00:00, 231.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import whisper\n",
    "import random\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "import evaluate\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text using Whisper's basic text normalizer.\"\"\"\n",
    "    return normalizer(text.strip())\n",
    "\n",
    "def compute_wer(reference: str, prediction: str) -> float:\n",
    "    \"\"\"Compute WER between two strings after normalization.\"\"\"\n",
    "    norm_ref = normalize_text(reference)\n",
    "    norm_pred = normalize_text(prediction)\n",
    "    return wer_metric.compute(references=[norm_ref], predictions=[norm_pred])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/fleurs\", \"lb_lu\")\n",
    "samples = dataset[\"test\"]\n",
    "\n",
    "prepared_samples = []\n",
    "\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "for sample in tqdm(samples):\n",
    "    audio_array = sample[\"audio\"][\"array\"]\n",
    "    sample_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "    reference = sample[\"transcription\"].strip()\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        audio_array = torchaudio.functional.resample(\n",
    "            torch.tensor(audio_array), orig_freq=sample_rate, new_freq=16000\n",
    "        ).numpy()\n",
    "\n",
    "    # Save to temp file\n",
    "    tmp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "    sf.write(tmp_file.name, audio_array, 16000)\n",
    "\n",
    "    prepared_samples.append({\n",
    "        \"path\": tmp_file.name,\n",
    "        \"reference\": reference\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Tun-Wellens/whisper-medium-lb-excluded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b8b4ad5fdc4944b493d73e03ce7a2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.28k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "516a9d81a3c1459e8e96144a5ed80d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load generation config from 'Tun-Wellens/whisper-medium-lb-excluded', using default config.\n",
      "Could not load processor from 'Tun-Wellens/whisper-medium-lb-excluded', using default processor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Whisper Eval: Tun-Wellens/whisper-medium-lb-excluded:   0%|          | 0/934 [00:00<?, ?it/s]Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Whisper Eval: Tun-Wellens/whisper-medium-lb-excluded:   1%|          | 10/934 [00:11<17:50,  1.16s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Whisper Eval: Tun-Wellens/whisper-medium-lb-excluded: 100%|██████████| 934/934 [20:09<00:00,  1.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER (Tun-Wellens/whisper-medium-lb-excluded) over 934 valid samples: 50.23%\n",
      "\n",
      "Sample predictions (Tun-Wellens/whisper-medium-lb-excluded):\n",
      "\n",
      "Reference : d'debatt gouf duerch kontroversen iwwer ausgabe fir hëllef an ëremopbau nom hurrikan katrina ausgeléist e puer finanzpolitesch konservativer hu se humorvoll als dem bush säin new-orleans-deal bezeechent\n",
      "Predicted : De Bat gouf duerch kontroversen iwwer Ausgabe fir Hëllef an erëmopbauen um Hurrikan Katrina ausgeléiste Profinanz, politesch konservativer, hu se humorfull als d'Ëm Bush säin New Orleans Deal bezeechent.\n",
      "WER       : 36.67%\n",
      "\n",
      "Reference : jugendlech ausräisser hunn eventuell schwéier kandsmësshandlung oder en trauma erlieft éier se opgi goufen oder fortgelaf sinn\n",
      "Predicted : Jugendlech Ausreisser hunn eventuell schwéier Kandesmisshandlung oder en Trauma erlieft, éier se opgi goufen oder fortbelaf sinn.\n",
      "WER       : 17.65%\n",
      "\n",
      "Reference : planze produzéieren sauerstoff deen d'mënschen ootmen a se huele kuelendioxid an dee mënschen ausootmen\n",
      "Predicted : D'Planze produzéiere Sauerstoff, deen e Mënsch boten ass, e huet en kuelen Dioxid, an deen e Mënsch esotot.\n",
      "WER       : 100.00%\n",
      "\n",
      "Reference : awer well et sech an den héijen tropen befënnt nëmmen e puer grad nërdlech vum äquator musst dir esouwuel mat hëtzt ëmmer a staarker sonn wann den himmer kloer ass méi seelen kloerkommen\n",
      "Predicted : Awer well et sech an den héijen Truppen befënnt nëmmen e puer grad nërdlech vum Ecuador, muss Dir esouwuel mat hëtzt ëmmer an staarke Sonnwand en Himmel kloer ass méi seelen kloerkomm.\n",
      "WER       : 30.30%\n",
      "\n",
      "Reference : seng aarbecht gëtt als vun esou unerkannter qualitéit an detailgenauegkeet gewierdegt datt en ee vun de ganz wéinegen allgemeng bekannten nimm ënner philatelisten ass e puer vun hinne spezialiséiere sech eleng op d'sammele vu senger aarbecht\n",
      "Predicted : Seng Aarbecht gëtt als vun esou unerkannt der Qualitéit an de Teil der naarbecht gëtt gewierdest, datt ee vun de ganz wéinegen allgemeng bekannten nim an ass de Villatelisten ass, e puer vun hir spezialiséiereg eleng op d'Zanglof vu sen Aarbecht.\n",
      "WER       : 51.35%\n",
      "\n",
      "\n",
      "Evaluating Tun-Wellens/whisper-medium-lb-included\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52cf4c507b5e483d869fb0e71cef9c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8127d20706d24425b3b0a9f530b676fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32ffaa398de487f98ae3c93306f63c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/3.75k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b818727f014e909a84c4f8217ccc2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/339 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c795551ed231440a9cb35b9846fa1d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/283k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97e3074741ca44c7abe121fee6a7a999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/836k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c819a057c47c29ee070596a712cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e56b09df815465296fa3cc0915ae0ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9822fc66daf14539821fc65a225bee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86860654172b42beb1e53459107a459d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/34.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c60986d1404800bf281ad5e14a53b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Whisper Eval: Tun-Wellens/whisper-medium-lb-included: 100%|██████████| 934/934 [19:37<00:00,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average WER (Tun-Wellens/whisper-medium-lb-included) over 934 valid samples: 49.23%\n",
      "\n",
      "Sample predictions (Tun-Wellens/whisper-medium-lb-included):\n",
      "\n",
      "Reference : de cuomo huet ufank vun dësem joer mat 53 säi gouverneursamt ugetrueden an de leschte mount e gesetzentworf fir d'legaliséierung vu gläichgeschlechtleche bestietnesser ënnerschriwwen\n",
      "Predicted : De Komer huet ufang vun dësen Joer mat 53 säi Gouverneursamt getrueden an de leschte Mount am Gesetz entwort fir d'Legaliséierung vu gläichgeschlechtlechtechten Bestietnesser ënnerschriwwen.\n",
      "WER       : 32.00%\n",
      "\n",
      "Reference : schliisslech ginn et vill kleng kazen inklusiv fräilafend hauskazen déi déi vill méi grouss zuel u klenge fäng ewéi insekten nagedéieren eidechsen a vulle friessen\n",
      "Predicted : Schlisslech ginn et vill kleng Kazen, inklusiv fräi lafen d'Hauskazen, déi déi vill méi grouss zur elo klenge Fange wéin ze akten, nagetéieren, eidacksen a Vullefriessen.\n",
      "WER       : 56.00%\n",
      "\n",
      "Reference : d'strategie huet sech als wierksam erwisen andeem se liewenswichteg militäresch an zivil liwwerungen ofgeschnidden huet obwuel dës blockad géint allgemeng akzeptéiert vëlkerrecht verstouss huet dat duerch méi international ofkommesse vun de leschten zwee joerhonnerte kodifizéiert gouf\n",
      "Predicted : D'Strategie huet sech als wirksam erwisen, andeem se e Liewenswichteg militäresch an civil Lieberungen ofgeschnidden huet, obwuel dëst Blakat géint allgemeng akzeptéiert Völkerrecht verstouss huet, dat duerch méi international Ofkommes se vun de leschten zwee Joerhonnerten kodifizéiert gouf.\n",
      "WER       : 27.03%\n",
      "\n",
      "Reference : den duvall dee bestuet ass a schonn zwee erwuesse kanner huet huet kee groussen androck beim miller hannerlooss mat deem d'geschicht an zesummenhang bruecht gouf\n",
      "Predicted : Den Duval, dee bestuet ass a schonn zwieweis de Kanner huet, huet kee groussen Androck bei Melanger los, mat deem d'Geschicht an zesummen Hannen bruecht gouf.\n",
      "WER       : 30.77%\n",
      "\n",
      "Reference : se hänkt domat zesummen ass awer normalerweis net mat alpinschi-änlechen touren oder biergsteige verbonnen woubäi dat lescht op géiem terrain stattfënnt a vill méi steif schi a schong erfuerdert\n",
      "Predicted : Se hänkt domat zesummen, ass awer normalerweis net am alpiniesch änlechen Touren oder Biergersteige verbonnen, woubäi dat d'lescht op géiem Terrain stattfonnt a vill méi stäif Schee aschong erfuerdert.\n",
      "WER       : 30.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import soundfile as sf\n",
    "from transformers import pipeline, WhisperProcessor, WhisperForConditionalGeneration, GenerationConfig\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# List of Whisper models fine-tuned on Luxembourgish\n",
    "whisper_model_names = [\n",
    "    \"Tun-Wellens/whisper-medium-lb-excluded\",\n",
    "    \"Tun-Wellens/whisper-medium-lb-included\",\n",
    "]\n",
    "\n",
    "for model_name in whisper_model_names:\n",
    "    print(f\"\\nEvaluating {model_name}\")\n",
    "\n",
    "    # Load model\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # Try to load generation config\n",
    "    try:\n",
    "        gen_config = GenerationConfig.from_pretrained(model_name)\n",
    "    except Exception:\n",
    "        print(f\"Could not load generation config from '{model_name}', using default config.\")\n",
    "        gen_config = GenerationConfig.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "    # Attach generation config to model\n",
    "    model.generation_config = gen_config\n",
    "\n",
    "    # Load processor with fallback\n",
    "    try:\n",
    "        processor = WhisperProcessor.from_pretrained(model_name)\n",
    "    except Exception:\n",
    "        print(f\"Could not load processor from '{model_name}', using default processor.\")\n",
    "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-medium\")\n",
    "\n",
    "    # Create ASR pipeline\n",
    "    asr_pipeline = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "    )\n",
    "\n",
    "    predictions_and_refs = []\n",
    "    wers = []\n",
    "    num_skipped = 0\n",
    "\n",
    "    for sample in tqdm(prepared_samples, desc=f\"Whisper Eval: {model_name}\"):\n",
    "        try:\n",
    "            # Load already-preprocessed 16kHz audio\n",
    "            speech_array, _ = sf.read(sample[\"path\"])\n",
    "\n",
    "            result = asr_pipeline({\n",
    "                \"array\": speech_array,\n",
    "                \"sampling_rate\": 16000\n",
    "            })\n",
    "            predicted_text = result[\"text\"].strip()\n",
    "\n",
    "            reference_text = sample[\"reference\"]\n",
    "            wer = compute_wer(reference_text, predicted_text)\n",
    "\n",
    "            predictions_and_refs.append((reference_text, predicted_text, wer))\n",
    "            wers.append(wer)\n",
    "\n",
    "        except ValueError as e:\n",
    "            if \"more than 3000 mel input features\" in str(e):\n",
    "                num_skipped += 1\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    if wers:\n",
    "        avg_wer = sum(wers) / len(wers)\n",
    "        print(f\"\\nAverage WER ({model_name}) over {len(wers)} valid samples: {avg_wer:.2%}\")\n",
    "    else:\n",
    "        print(f\"\\nNo valid samples for {model_name}\")\n",
    "\n",
    "    if num_skipped:\n",
    "        print(f\"Skipped {num_skipped} sample(s) due to long-form constraint (>30s)\")\n",
    "\n",
    "    if predictions_and_refs:\n",
    "        print(f\"\\nSample predictions ({model_name}):\\n\")\n",
    "        for ref, pred, err in random.sample(predictions_and_refs, min(5, len(predictions_and_refs))):\n",
    "            print(f\"Reference : {ref}\")\n",
    "            print(f\"Predicted : {pred}\")\n",
    "            print(f\"WER       : {err:.2%}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
