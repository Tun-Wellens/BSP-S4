{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b30e23d35ed540b1839ee998225169f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.49k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for google/fleurs contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/google/fleurs\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae5d78796e44c619413059909034ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/12.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40936b44e5d9408c8617493cdccf8c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/13.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de594a3d84945b39470c063630206e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.41G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3b586c7d5f43c59b7a23f9bbb1dff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/191M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dfeff86d6444b12a7025d9763652433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4622aa720d344e1b9e37652b2902a303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.50M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d1ba1744894482b5209302aca3a9e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/245k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc228d96ce6b42aeb5036746af0f7c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/593k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8829961a68c348e68a2e4a2af9956be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9998e350e64679871da5e2f2e85d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d506dde7eb8b4284b44b6fbe9ead671a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 934/934 [00:15<00:00, 61.63it/s] \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import whisper\n",
    "import random\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "from transformers.models.whisper.english_normalizer import BasicTextNormalizer\n",
    "import evaluate\n",
    "\n",
    "normalizer = BasicTextNormalizer()\n",
    "wer_metric = evaluate.load(\"wer\")\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text using Whisper's basic text normalizer.\"\"\"\n",
    "    return normalizer(text.strip())\n",
    "\n",
    "def compute_wer(reference: str, prediction: str) -> float:\n",
    "    \"\"\"Compute WER between two strings after normalization.\"\"\"\n",
    "    norm_ref = normalize_text(reference)\n",
    "    norm_pred = normalize_text(prediction)\n",
    "    return wer_metric.compute(references=[norm_ref], predictions=[norm_pred])\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"google/fleurs\", \"lb_lu\")\n",
    "samples = dataset[\"test\"]\n",
    "\n",
    "prepared_samples = []\n",
    "\n",
    "import tempfile\n",
    "import soundfile as sf\n",
    "import torch\n",
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "\n",
    "for sample in tqdm(samples):\n",
    "    audio_array = sample[\"audio\"][\"array\"]\n",
    "    sample_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "    reference = sample[\"transcription\"].strip()\n",
    "\n",
    "    # Resample if necessary\n",
    "    if sample_rate != 16000:\n",
    "        audio_array = torchaudio.functional.resample(\n",
    "            torch.tensor(audio_array), orig_freq=sample_rate, new_freq=16000\n",
    "        ).numpy()\n",
    "\n",
    "    # Save to temp file\n",
    "    tmp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "    sf.write(tmp_file.name, audio_array, 16000)\n",
    "\n",
    "    prepared_samples.append({\n",
    "        \"path\": tmp_file.name,\n",
    "        \"reference\": reference\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 2.87G/2.87G [00:27<00:00, 113MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model: large-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating large-v2: 100%|██████████| 934/934 [49:05<00:00,  3.15s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[large-v2] Average WER: 92.92%\n",
      "Reference: de service gëtt dacks vun der schëfffaart inklusiv sportbooter esouwéi expeditioune verwent déi bedarf un externen daten a sproocherkennung hunn\n",
      "Predicted: Des servies gedags von der Schöf-Arst inklusive Spurtboote sowie Expeditione verwandt de Bedrof on externen Daten o spurtakannung hon.\n",
      "WER: 90.00%\n",
      "\n",
      "Reference: am waarme klima vum noen oste waren haiser net esou wichteg\n",
      "Predicted: Am warme klima vum Nooën-Osten waren Heusan net so wichtig.\n",
      "WER: 54.55%\n",
      "\n",
      "Reference: dat sinn heiansdo iwwerfëllt familljestränn mat enger gudder auswiel u butteker laanscht d'küst schwammen ass sécher\n",
      "Predicted: WHEN nooit inderdaad aara trimmed je je ap' aa stik et zoom je ergap, SUBUSIEERDE AARE\n",
      "WER: 100.00%\n",
      "\n",
      "Reference: verkéiersfloss ass d'untersuchung vun der beweegung vun eenzele chaufferen a gefierer tëschent zwee punkten an d'wiesselwierkungen déi se openeen hunn\n",
      "Predicted: Tvrkaja flos ast unta sorung vonder bewegjung von enzele shofören and gefejrd tswischen twee pumten and vierselvirkungen die saoben hen hon.\n",
      "WER: 100.00%\n",
      "\n",
      "Reference: laut der japanescher atomagence goufe radioaktiven cäsium a jod op der wierksanlag identifizéiert\n",
      "Predicted: Laut der Japanische Atomagentur von Radioaktiv Késium, haut der Wirksanlage die Infizierte.\n",
      "WER: 84.62%\n",
      "\n",
      "Loaded model: large-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating large-v3: 100%|██████████| 934/934 [29:55<00:00,  1.92s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[large-v3] Average WER: 85.76%\n",
      "Reference: nuets goufen tëschent 150 bis 200 kopië gemaach déi haut als dunlap broadsides bekannt sinn\n",
      "Predicted: Nuts giffen tischen 250 bis 200 Kopiergema die Haut als Dunlop Broad Sides bekannt ging.\n",
      "WER: 73.33%\n",
      "\n",
      "Reference: d'kuuscht ass op der zougewanter säit ongeféier 70 km an op der ofgewanter säit ongeféier 100 km déck\n",
      "Predicted: Die Kuh ist op der zugewandte Seite ungefähr 70 km an, auf der aufgewandte Seite ungefähr 100 km dick.\n",
      "WER: 57.89%\n",
      "\n",
      "Reference: virun allem gëtt behaapt datt een erkenne kann ob eng persoun litt andeem ee mikroausdréck richteg interpretéiert\n",
      "Predicted: Für in allem geht behaabt, dat eh noch keine Kern ob beim Personenlid, an dem ihr Mikroeisträg richtig interpretiert.\n",
      "WER: 100.00%\n",
      "\n",
      "Reference: am norden an einfach ze erreechen befënnt sech d'romantesch a faszinéierend stad sintra déi no enger feiereger duerstellung vun hirer pruecht déi vum lord byron opgezeechent gouf fir auslänner berüümt gouf\n",
      "Predicted: Am Norden an Einfahrt zur Arischen befent sich dromantisch a fascinerend Staatsintra denueinger Fährischer durchsteigend von hierer Prüchte vom Lord Byron abgzichtendgöf fie Ausländer über Rindgöf.\n",
      "WER: 81.25%\n",
      "\n",
      "Reference: de service gëtt dacks vun der schëfffaart inklusiv sportbooter esouwéi expeditioune verwent déi bedarf un externen daten a sproocherkennung hunn\n",
      "Predicted: Des service gedags von der Schifffahrt inklusiv Sportoboter sowie Expeditionen verwandt die Bedarf von externen Daten aus Spruchekanonen.\n",
      "WER: 70.00%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_names = [\"large-v2\", \"large-v3\"]\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        model = whisper.load_model(model_name)\n",
    "        print(f\"Loaded model: {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Model {model_name} not available: {e}\")\n",
    "        continue  # Skip to next model if this one fails\n",
    "\n",
    "    refs_preds = []\n",
    "    wers = []\n",
    "\n",
    "    for sample in tqdm(prepared_samples, desc=f\"Evaluating {model_name}\"):\n",
    "        try:\n",
    "            result = model.transcribe(sample[\"path\"], language=\"lb\", task=\"transcribe\")\n",
    "            prediction = result[\"text\"].strip()\n",
    "\n",
    "            error = compute_wer(sample[\"reference\"], prediction)\n",
    "            wers.append(error)\n",
    "            refs_preds.append((sample[\"reference\"], prediction, error))\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if \"CUDA out of memory\" in str(e):\n",
    "                print(f\"CUDA OOM on: {sample['path']}. Skipping.\")\n",
    "            else:\n",
    "                raise e\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if wers:\n",
    "        average_wer = sum(wers) / len(wers)\n",
    "        print(f\"[{model_name}] Average WER: {average_wer:.2%}\")\n",
    "    else:\n",
    "        print(f\"[{model_name}] No valid WER results.\")\n",
    "\n",
    "    for ref, pred, err in random.sample(refs_preds, min(5, len(refs_preds))):\n",
    "        print(f\"Reference: {ref}\")\n",
    "        print(f\"Predicted: {pred}\")\n",
    "        print(f\"WER: {err:.2%}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
